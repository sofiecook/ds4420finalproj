{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "708e2cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#!pip install textblob\n",
    "#pip install wordcloud matplotlib\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!pip install ucimlrepo\n",
    "#!pip install scikit-learn\n",
    "#pip install seaborn\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52b7d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d63ae81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "drug_reviews_drugs_com = fetch_ucirepo(id=462)\n",
    "drug_reviews_druglib_com = fetch_ucirepo(id=461)\n",
    "\n",
    "# Load the datasets\n",
    "df1 = drug_reviews_drugs_com.data.features\n",
    "df2 = drug_reviews_druglib_com.data.features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77bfe3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5add676",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f8d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df.head())\n",
    "print(combined_df.info())\n",
    "print(\"Unique Drug Names:\", combined_df['urlDrugName'].nunique())\n",
    "print(\"Unique Conditions:\", combined_df['condition'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7197d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df.describe())\n",
    "print(combined_df['rating'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e927bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert reviews to string and calculate lengths\n",
    "#converts review data into string format calculates the length of each review, stores those lengths in new columns\n",
    "combined_df['benefitsLength'] = combined_df['benefitsReview'].astype(str).apply(len)\n",
    "combined_df['sideEffectsLength'] = combined_df['sideEffectsReview'].astype(str).apply(len)\n",
    "combined_df['commentsLength'] = combined_df['commentsReview'].astype(str).apply(len)\n",
    "\n",
    "# Display descriptive statistics (lengths)\n",
    "print(combined_df[['benefitsLength', 'sideEffectsLength', 'commentsLength']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a8dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for numerical columns\n",
    "print(combined_df.describe())\n",
    "\n",
    "# distribution of the rating column\n",
    "print(combined_df['rating'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4632486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 8))\n",
    "sns.countplot(data=combined_df, x='rating', palette='viridis')\n",
    "plt.title('Distribution of Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468aa168",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 5))\n",
    "sns.countplot(data=combined_df, x='effectiveness', palette='plasma')\n",
    "plt.title('Effectiveness Distribution')\n",
    "plt.xlabel('Effectiveness')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95103c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = combined_df.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be1a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Determine the top N useful review counts USING TOP #10\n",
    "top_n = 10 \n",
    "top_useful_counts = combined_df['usefulCount'].value_counts().nlargest(top_n)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# the top N useful reviews\n",
    "sns.countplot(data=combined_df[combined_df['usefulCount'].isin(top_useful_counts.index)],\n",
    "                            x='usefulCount', palette='Set2', order=top_useful_counts.index)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Distribution of Top Useful Reviews', fontsize=16)\n",
    "plt.xlabel('Useful Reviews Count', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)  # grid for better readability\n",
    "plt.tight_layout()  \n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9ca61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df):\n",
    "    df = df.rename(columns={\n",
    "        'urlDrugName': 'drug_name',\n",
    "        'sideEffects': 'side_effects',\n",
    "        'benefitsReview': 'benefits_review',\n",
    "        'sideEffectsReview': 'side_effects_review',\n",
    "        \n",
    "    })\n",
    "    return df\n",
    "\n",
    "df1 = rename_columns(df1)\n",
    "df2 = rename_columns(df2)\n",
    "\n",
    "# Check the column names\n",
    "print(\"Columns in Dataset 1:\", df1.columns)\n",
    "print(\"Columns in Dataset 2:\", df2.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d34589",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_drug_ratings(df, dataset_name, top_n=30):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Check if drug_name exists \n",
    "    if 'drug_name' in df.columns:\n",
    "        # Get the top N drugs based on count\n",
    "        top_drugs = df['drug_name'].value_counts().nlargest(top_n).index\n",
    "        sns.countplot(data=df[df['drug_name'].isin(top_drugs)], x='drug_name', order=top_drugs)\n",
    "        plt.title(f'Top {top_n} Drug Ratings Count in {dataset_name}')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"'drug_name' column not found in {dataset_name}.\")\n",
    "\n",
    "# Plot for both datasets\n",
    "plot_drug_ratings(df1, \"Dataset 1\", top_n=30)\n",
    "plot_drug_ratings(df2, \"Dataset 2\", top_n=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_cloud(df, column_name, dataset_name):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Check if the column exists\n",
    "    if column_name in df.columns:\n",
    "        # Combine all reviews into a single string\n",
    "        all_reviews = ' '.join(df[column_name].dropna())\n",
    "        \n",
    "        # Generate the word cloud\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_reviews)\n",
    "        \n",
    "        # Display the word cloud\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')  # Hide axes\n",
    "        plt.title(f'Word Cloud for {column_name} in {dataset_name}')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"'{column_name}' column not found in {dataset_name}.\")\n",
    "\n",
    "#  word clouds for both datasets\n",
    "plot_word_cloud(df1, 'benefits_review', \"Dataset 1\")\n",
    "plot_word_cloud(df2, 'benefits_review', \"Dataset 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be4ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_drug_word_cloud(df, dataset_name):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Check if 'drug_name' column exists\n",
    "    if 'drug_name' in df.columns:\n",
    "        # Combine all drug names into a single string\n",
    "        all_drugs = ' '.join(df['drug_name'].dropna())\n",
    "        \n",
    "        # Generate the word cloud\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_drugs)\n",
    "        \n",
    "        # Display the word cloud\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')  # Hide axes\n",
    "        plt.title(f'Word Cloud of Drug Names in {dataset_name}')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"'drug_name' column not found in {dataset_name}.\")\n",
    "\n",
    "# Create word clouds for drug names in both datasets\n",
    "plot_drug_word_cloud(df1, \"Dataset 1\")\n",
    "plot_drug_word_cloud(df2, \"Dataset 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a26023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_frequency(df, column_name):\n",
    "    if column_name in df.columns:\n",
    "        all_reviews = ' '.join(df[column_name].dropna())\n",
    "        words = all_reviews.split()\n",
    "        word_freq = pd.Series(words).value_counts().head(20)  # Top 20 words\n",
    "        word_freq.plot(kind='bar', figsize=(12, 6))\n",
    "        plt.title(f'Top 20 Words in {column_name}')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"'{column_name}' column not found in the DataFrame.\")\n",
    "\n",
    "# Plot for benefits reviews\n",
    "plot_word_frequency(df1, 'benefits_review')\n",
    "plot_word_frequency(df2, 'benefits_review')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdfc60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word cloud for conditions in df1\n",
    "condition_text = ' '.join(df1['condition'].dropna())\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(condition_text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Common Conditions in Reviews (Dataset 1)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb2f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word cloud for side effects in df2\n",
    "side_effects_text = ' '.join(df2['side_effects_review'].dropna())  # Use df2 for Dataset 2\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(side_effects_text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Common Side Effects in Reviews (Dataset 2)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b273daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word cloud for conditions in df2\n",
    "condition_text = ' '.join(df2['condition'].dropna())  # Use df2 and focus on 'condition' column\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(condition_text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Common Conditions in Reviews (Dataset 2)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b97c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text (remove punctuation, lowercase, remove numbers)\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()  \n",
    "        text = re.sub(r'\\d+', '', text)  \n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the review column\n",
    "df1['cleaned_review'] = df1['review'].apply(clean_text)\n",
    "\n",
    "# Check cleaned reviews\n",
    "print(\"Cleaned review:\", df1['cleaned_review'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aeabf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')  \n",
    "# Set of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean text \n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()  \n",
    "        text = re.sub(r'\\d+', '', text)  \n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Function to tokenize and remove stopwords\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    if isinstance(text, str):\n",
    "        words = word_tokenize(text)\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        return words\n",
    "    return []  # Return an empty list if text is not a string\n",
    "\n",
    "# Clean the review column\n",
    "df1['cleaned_review'] = df1['review'].apply(clean_text)\n",
    "\n",
    "# Apply tokenization and stopword removal\n",
    "df1['tokens_review'] = df1['cleaned_review'].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "# tokenized data\n",
    "print(\"Tokenized review:\", df1['tokens_review'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d443eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize words\n",
    "def lemmatize_words(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Apply lemmatization\n",
    "df1['lemmatized_review'] = df1['tokens_review'].apply(lemmatize_words)\n",
    "\n",
    "# lemmatized data\n",
    "print(\"Lemmatized review:\", df1['lemmatized_review'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0538750b-4a74-4678-9f3d-c9a913827397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the lemmatized review column\n",
    "if 'lemmatized_review' in df1.columns:\n",
    "    # Use the entire dataset for TF-IDF\n",
    "    text_column = df1['lemmatized_review'].astype(str)\n",
    "    \n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(text_column)\n",
    "\n",
    "    # Display the shape of the TF-IDF matrix\n",
    "    print(\"TF-IDF matrix shape:\", X_tfidf.shape)\n",
    "else:\n",
    "    print(\"'lemmatized_review' is missing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa550d-02b2-43f7-ad4a-10e1ed8f6307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Check for the lemmatized review column\n",
    "if 'lemmatized_review' in df1.columns:\n",
    "    # Use the entire dataset for TF-IDF\n",
    "    text_column = df1['lemmatized_review'].astype(str)\n",
    "    \n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(text_column)\n",
    "\n",
    "    # Display the shape of the TF-IDF matrix\n",
    "    print(\"TF-IDF matrix shape:\", X_tfidf.shape)\n",
    "\n",
    "    # Convert to DataFrame for easier analysis\n",
    "    tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "    \n",
    "    # Display top terms by their mean TF-IDF score\n",
    "    mean_tfidf_scores = tfidf_df.mean().sort_values(ascending=False)\n",
    "    print(\"Top TF-IDF terms:\\n\", mean_tfidf_scores.head(10))\n",
    "\n",
    "    # Visualize the top TF-IDF terms\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=mean_tfidf_scores.head(10).values, y=mean_tfidf_scores.head(10).index)\n",
    "    plt.title('Top 10 TF-IDF Terms')\n",
    "    plt.xlabel('Mean TF-IDF Score')\n",
    "    plt.ylabel('Terms')\n",
    "    plt.show()\n",
    "\n",
    "    # Dimensionality reduction using PCA for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_data = pca.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "    # Create a DataFrame for the PCA result\n",
    "    pca_df = pd.DataFrame(reduced_data, columns=['PC1', 'PC2'])\n",
    "\n",
    "    # Visualize PCA results\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x='PC1', y='PC2', data=pca_df, alpha=0.5)\n",
    "    plt.title('PCA of TF-IDF Features')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"'lemmatized_review' is missing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d57a41-7600-4232-8325-aa88e48056b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get sentiment score\n",
    "def analyze_sentiment(text):\n",
    "    if isinstance(text, str):\n",
    "        return sia.polarity_scores(text)\n",
    "    else:\n",
    "        return {'neg': 0, 'neu': 0, 'pos': 0, 'compound': 0}\n",
    "\n",
    "# Ensure that you have a column with the raw reviews, for example, 'review'\n",
    "if 'review' in df1.columns:\n",
    "    # Apply sentiment analysis to the raw review column\n",
    "    df1['comments_review_sentiment'] = df1['review'].apply(analyze_sentiment)\n",
    "\n",
    "    # Extract compound sentiment score\n",
    "    df1['comments_sentiment_score'] = df1['comments_review_sentiment'].apply(lambda x: x['compound'])\n",
    "\n",
    "    # Check the distribution of sentiment scores\n",
    "    print(\"Sentiment score distribution:\")\n",
    "    print(df1['comments_sentiment_score'].describe())\n",
    "\n",
    "    # Print sentiment scores for the top 30 rows\n",
    "    print(\"Sentiment scores for comments reviews:\")\n",
    "    print(df1[['review', 'comments_sentiment_score']].head(30))\n",
    "\n",
    "    # Identify reviews with significant sentiment scores\n",
    "    significant_sentiment = df1[df1['comments_sentiment_score'].abs() > 0.1]  # Adjust the threshold as needed\n",
    "    print(\"Reviews with significant sentiment scores:\")\n",
    "    print(significant_sentiment[['review', 'comments_sentiment_score']].head(30))\n",
    "else:\n",
    "    print(\"'review' column is missing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0a46f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get sentiment score\n",
    "def analyze_sentiment(text):\n",
    "    if isinstance(text, str):\n",
    "        return sia.polarity_scores(text)\n",
    "    else:\n",
    "        return {'neg': 0, 'neu': 0, 'pos': 0, 'compound': 0}\n",
    "\n",
    "# Apply sentiment analysis to the entire DataFrame\n",
    "df1['comments_review_sentiment'] = df1['lemmatized_review'].apply(analyze_sentiment)\n",
    "\n",
    "# Extract compound sentiment score\n",
    "df1['comments_sentiment_score'] = df1['comments_review_sentiment'].apply(lambda x: x['compound'])\n",
    "\n",
    "# Check the distribution of sentiment scores\n",
    "print(\"Sentiment score distribution:\")\n",
    "print(df1['comments_sentiment_score'].describe())\n",
    "\n",
    "# Print sentiment scores for the top 30 rows\n",
    "print(\"Sentiment scores for comments reviews:\")\n",
    "print(df1[['lemmatized_review', 'comments_sentiment_score']].head(15))\n",
    "\n",
    "# Identify reviews with significant sentiment scores\n",
    "significant_sentiment = df1[df1['comments_sentiment_score'].abs() > 0.1]  # Adjust the threshold as needed\n",
    "print(\"Reviews with significant sentiment scores:\")\n",
    "print(significant_sentiment[['lemmatized_review', 'comments_sentiment_score']].head(15))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26414d7-3168-4bf7-b7b2-621d620eb515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#  'review' column exists\n",
    "if 'review' in df1.columns:\n",
    "    # Use TF-IDF for feature extraction\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(df1['review'].astype(str))\n",
    "\n",
    "    # Apply K-means clustering\n",
    "    n_clusters = 5  # Adjust the number of clusters as needed\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    df1['cluster'] = kmeans.fit_predict(X_tfidf)\n",
    "\n",
    "    # Print cluster assignments\n",
    "    print(\"Cluster assignments for reviews:\")\n",
    "    print(df1[['review', 'cluster']].head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6752d-71b0-4ddd-8ac1-859824f8b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for lemmatized review column\n",
    "if 'lemmatized_review' in df1.columns:\n",
    "    # Use TF-IDF for feature extraction\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(df1['lemmatized_review'].astype(str))\n",
    "\n",
    "    # Apply K-means clustering\n",
    "    n_clusters = 5  # Adjust the number of clusters as needed\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    df1['cluster'] = kmeans.fit_predict(X_tfidf)\n",
    "\n",
    "    # Print cluster assignments\n",
    "    print(\"Cluster assignments for reviews:\")\n",
    "    print(df1[['lemmatized_review', 'cluster']].head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4098b-e49a-46fb-ad30-6acd1c098e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9270b0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_most_frequent_words(text_column, top_n=20):\n",
    "    # Use CountVectorizer to get word frequencies\n",
    "    vectorizer = CountVectorizer(stop_words='english', max_features=top_n)\n",
    "    word_matrix = vectorizer.fit_transform(text_column)\n",
    "    word_freq = np.asarray(word_matrix.sum(axis=0)).flatten()\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create a DataFrame for plotting\n",
    "    word_freq_dict = dict(zip(words, word_freq))\n",
    "    sorted_words = sorted(word_freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    word_df = pd.DataFrame(sorted_words, columns=['Word', 'Frequency'])\n",
    "\n",
    "    # Plot the results\n",
    "    sns.barplot(x='Frequency', y='Word', data=word_df)\n",
    "    plt.title(f'Top {top_n} Most Frequent Words')\n",
    "    plt.show()\n",
    "\n",
    "# Check for the original review column\n",
    "if 'lemmatized_review' in df1.columns:  \n",
    "    # Plot the most frequent words in the entire dataset\n",
    "    plot_most_frequent_words(df1['lemmatized_review'].astype(str), top_n=25)\n",
    "else:\n",
    "    print(\"'lemmatized_review' is missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e4ac91-df57-482c-bbda-085bd391c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Check if necessary columns exist\n",
    "if 'lemmatized_review' in df1.columns:  \n",
    "    # Optionally limit the size of the dataset for faster processing\n",
    "    max_rows = 100000  # Adjust this as needed\n",
    "    text_column = df1['lemmatized_review'].astype(str).head(max_rows)  # Use only the top 'max_rows'\n",
    "\n",
    "    # Use CountVectorizer for LDA with a limit on the number of features\n",
    "    vectorizer = CountVectorizer(stop_words='english', max_features=300)  # Reduce max_features further\n",
    "    X = vectorizer.fit_transform(text_column)\n",
    "\n",
    "    # Apply LDA for topic modeling with fewer topics\n",
    "    lda = LatentDirichletAllocation(n_components=3, random_state=42, n_jobs=-1)  # Use all available cores\n",
    "    lda.fit(X)\n",
    "\n",
    "    # Function to display top words in each topic\n",
    "    def print_lda_topics(lda, vectorizer, top_n=10):\n",
    "        words = vectorizer.get_feature_names_out()\n",
    "        for i, topic in enumerate(lda.components_):\n",
    "            print(f\"Topic {i + 1}:\")\n",
    "            print([words[i] for i in topic.argsort()[-top_n:]])\n",
    "\n",
    "    # Print the top words for each topic\n",
    "    print_lda_topics(lda, vectorizer)\n",
    "else:\n",
    "    print(\"'lemmatized_review' is missing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff0769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\d+', '', text) \n",
    "        text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    return text\n",
    "\n",
    "# Clean the benefits_review and commentsReview columns\n",
    "df2['cleaned_benefits_review'] = df2['benefits_review'].apply(clean_text)\n",
    "df2['cleaned_comments_review'] = df2['commentsReview'].apply(clean_text)\n",
    "\n",
    "# Check the cleaned columns\n",
    "print(df2[['cleaned_benefits_review', 'cleaned_comments_review']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ccb212-63ea-4b7e-9dd1-c6f7c1d977a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    if isinstance(text, str):\n",
    "        words = word_tokenize(text)\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        return words\n",
    "    return []\n",
    "\n",
    "# Apply tokenization\n",
    "df2['tokens_benefits_review'] = df2['cleaned_benefits_review'].apply(tokenize_and_remove_stopwords)\n",
    "df2['tokens_comments_review'] = df2['cleaned_comments_review'].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "# Check the tokenized columns\n",
    "print(df2[['tokens_benefits_review', 'tokens_comments_review']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae38c02b-a0ea-49fd-9d46-c30d9e914e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lemmatization function\n",
    "def lemmatize_words(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Apply lemmatization\n",
    "df2['lemmatized_benefits_review'] = df2['tokens_benefits_review'].apply(lemmatize_words)\n",
    "df2['lemmatized_comments_review'] = df2['tokens_comments_review'].apply(lemmatize_words)\n",
    "\n",
    "# Check the lemmatized columns\n",
    "print(df2[['lemmatized_benefits_review', 'lemmatized_comments_review']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a14423-7561-4db5-89a3-5bf43db08cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Analyze sentiment function\n",
    "def analyze_sentiment(tokens):\n",
    "    text = ' '.join(tokens)\n",
    "    return sia.polarity_scores(text)\n",
    "\n",
    "# Apply sentiment analysis to comments review\n",
    "df2['comments_review_sentiment'] = df2['lemmatized_comments_review'].apply(analyze_sentiment)\n",
    "df2['comments_sentiment_score'] = df2['comments_review_sentiment'].apply(lambda x: x['compound'])\n",
    "\n",
    "# Check sentiment analysis results\n",
    "print(df2[['comments_sentiment_score', 'comments_review_sentiment']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d928cc8-29ec-41ea-8a8b-e808ef57c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot most frequent words\n",
    "def plot_most_frequent_words(text_column, top_n=15):\n",
    "    vectorizer = CountVectorizer(stop_words='english', max_features=top_n)\n",
    "    word_matrix = vectorizer.fit_transform(text_column)\n",
    "    word_freq = np.asarray(word_matrix.sum(axis=0)).flatten()\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "\n",
    "    word_freq_dict = dict(zip(words, word_freq))\n",
    "    sorted_words = sorted(word_freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    word_df = pd.DataFrame(sorted_words, columns=['Word', 'Frequency'])\n",
    "\n",
    "    sns.barplot(x='Frequency', y='Word', data=word_df)\n",
    "    plt.title(f'Top {top_n} Most Frequent Words')\n",
    "    plt.show()\n",
    "\n",
    "# Analyze the whole dataset\n",
    "if 'cleaned_benefits_review' in df2.columns:\n",
    "    plot_most_frequent_words(df2['cleaned_benefits_review'].astype(str), top_n=20)\n",
    "else:\n",
    "    print(\"'cleaned_benefits_review' is missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ca15d6-d9ab-4693-86ec-6a3c3838053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modeling on lemmatized benefits review\n",
    "if 'lemmatized_benefits_review' in df2.columns:\n",
    "    # Use the entire dataset instead of sampling\n",
    "    text_column = df2['lemmatized_benefits_review'].astype(str)\n",
    "\n",
    "    # Create the CountVectorizer\n",
    "    vectorizer = CountVectorizer(stop_words='english', max_features=500)\n",
    "    X = vectorizer.fit_transform(text_column)\n",
    "\n",
    "    # Apply LDA for topic modeling\n",
    "    lda = LatentDirichletAllocation(n_components=3, random_state=42) \n",
    "    lda.fit(X)\n",
    "\n",
    "    # Function to display top words in each topic\n",
    "    def print_lda_topics(lda, vectorizer, top_n=10):\n",
    "        words = vectorizer.get_feature_names_out()\n",
    "        for i, topic in enumerate(lda.components_):\n",
    "            print(f\"Topic {i + 1}:\")\n",
    "            print([words[i] for i in topic.argsort()[-top_n:]])\n",
    "\n",
    "    # Display the topics\n",
    "    print_lda_topics(lda, vectorizer)\n",
    "else:\n",
    "    print(\"The required column 'lemmatized_benefits_review' is missing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4905e38-3782-40d3-aa08-1769701cb6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to perform NER on a batch of texts\n",
    "def extract_entities_batch(texts):\n",
    "    docs = nlp.pipe(texts, batch_size=50)  # Process in batches of 50\n",
    "    return [[(ent.text, ent.label_) for ent in doc.ents] for doc in docs]\n",
    "\n",
    "# Apply NER to df1 (assuming df1 is already defined)\n",
    "df1['entities'] = extract_entities_batch(df1['review'].astype(str).tolist())\n",
    "\n",
    "# Apply NER to df2 (assuming df2 is already defined)\n",
    "df2['entities'] = extract_entities_batch(df2['review'].astype(str).tolist())\n",
    "\n",
    "# Display the entities found in the top 30 reviews of df1\n",
    "print(\"Entities extracted from df1 reviews:\")\n",
    "print(df1[['review', 'entities']].head(30))\n",
    "\n",
    "# Display the entities found in the top 30 reviews of df2\n",
    "print(\"Entities extracted from df2 reviews:\")\n",
    "print(df2[['review', 'entities']].head(30))\n",
    "\n",
    "# Count the frequency of key entities\n",
    "from collections import Counter\n",
    "\n",
    "def count_entities(df):\n",
    "    entity_list = []\n",
    "    for entities in df['entities']:\n",
    "        entity_list.extend([ent[0] for ent in entities])\n",
    "    return Counter(entity_list)\n",
    "\n",
    "# Count entities in both DataFrames\n",
    "df1_entities_count = count_entities(df1)\n",
    "df2_entities_count = count_entities(df2)\n",
    "\n",
    "# Display the most common entities in df1\n",
    "print(\"Most common entities in df1:\")\n",
    "print(df1_entities_count.most_common(10))\n",
    "\n",
    "# Display the most common entities in df2\n",
    "print(\"Most common entities in df2:\")\n",
    "print(df2_entities_count.most_common(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
